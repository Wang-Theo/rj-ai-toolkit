"""
æ–‡æœ¬åˆ†æå·¥å…·æ¨¡å—

æä¾›å„ç§æ–‡æœ¬åˆ†æåŠŸèƒ½ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬ã€‚
"""

import re
from typing import Dict, List, Tuple
from langchain_core.tools import Tool
from collections import Counter


class TextAnalyzer:
    """æ–‡æœ¬åˆ†æå™¨ç±»"""
    
    @staticmethod
    def analyze_basic_stats(text: str) -> Dict[str, int]:
        """
        åˆ†ææ–‡æœ¬åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        if not text or not text.strip():
            return {}
        
        # åŸºç¡€ç»Ÿè®¡
        total_chars = len(text)
        chars_no_spaces = len(text.replace(' ', ''))
        lines = text.split('\n')
        line_count = len(lines)
        
        # è¯æ±‡åˆ†æ
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        english_words = re.findall(r'[a-zA-Z]+', text)
        numbers = re.findall(r'\d+', text)
        
        # å¥å­ç»Ÿè®¡
        sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
        sentence_count = sum(text.count(ending) for ending in sentence_endings)
        
        # æ®µè½ç»Ÿè®¡ï¼ˆè¿ç»­éç©ºè¡Œä¸ºä¸€æ®µï¼‰
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        paragraph_count = len(paragraphs)
        
        return {
            'total_chars': total_chars,
            'chars_no_spaces': chars_no_spaces,
            'line_count': line_count,
            'chinese_char_count': len(chinese_chars),
            'english_word_count': len(english_words),
            'number_count': len(numbers),
            'sentence_count': sentence_count,
            'paragraph_count': paragraph_count,
        }
    
    @staticmethod
    def analyze_word_frequency(text: str, top_n: int = 10) -> List[Tuple[str, int]]:
        """
        åˆ†æè¯é¢‘
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            top_n: è¿”å›å‰Nä¸ªé«˜é¢‘è¯
            
        Returns:
            è¯é¢‘åˆ—è¡¨
        """
        # æå–è‹±æ–‡å•è¯
        english_words = re.findall(r'[a-zA-Z]+', text.lower())
        # æå–ä¸­æ–‡å­—ç¬¦ï¼ˆç®€å•æŒ‰å­—ç¬¦åˆ†è¯ï¼‰
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        
        # åˆå¹¶æ‰€æœ‰è¯æ±‡
        all_words = english_words + chinese_chars
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(all_words)
        return word_freq.most_common(top_n)
    
    @staticmethod
    def detect_language(text: str) -> Dict[str, float]:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€æ¯”ä¾‹
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            
        Returns:
            è¯­è¨€æ¯”ä¾‹å­—å…¸
        """
        if not text:
            return {}
        
        total_chars = len(re.sub(r'\s', '', text))  # ä¸è®¡ç©ºæ ¼
        if total_chars == 0:
            return {}
        
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        numbers = len(re.findall(r'\d', text))
        punctuation = len(re.findall(r'[^\w\s\u4e00-\u9fff]', text))
        
        return {
            'chinese_ratio': chinese_chars / total_chars,
            'english_ratio': english_chars / total_chars,
            'number_ratio': numbers / total_chars,
            'punctuation_ratio': punctuation / total_chars
        }


def create_text_analyzer_tool() -> Tool:
    """
    åˆ›å»ºæ–‡æœ¬åˆ†æå·¥å…·
    
    Returns:
        LangChain Toolå¯¹è±¡
    """
    def analyze_text(text: str) -> str:
        """
        åˆ†ææ–‡æœ¬çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            
        Returns:
            æ ¼å¼åŒ–çš„åˆ†æç»“æœ
        """
        if not text or not text.strip():
            return "é”™è¯¯ï¼šè¯·æä¾›æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹"
        
        try:
            # åŸºç¡€ç»Ÿè®¡
            stats = TextAnalyzer.analyze_basic_stats(text)
            
            # è¯é¢‘åˆ†æ
            word_freq = TextAnalyzer.analyze_word_frequency(text, 5)
            
            # è¯­è¨€æ£€æµ‹
            lang_ratios = TextAnalyzer.detect_language(text)
            
            # è®¡ç®—å¹³å‡å€¼
            total_words = stats['chinese_char_count'] + stats['english_word_count']
            avg_word_length = stats['chars_no_spaces'] / total_words if total_words > 0 else 0
            
            # æ ¼å¼åŒ–è¾“å‡º
            result = f"""ğŸ“Š æ–‡æœ¬åˆ†æç»“æœ:

ğŸ”¢ åŸºç¡€ç»Ÿè®¡:
- æ€»å­—ç¬¦æ•°: {stats['total_chars']}
- å­—ç¬¦æ•°(ä¸å«ç©ºæ ¼): {stats['chars_no_spaces']}
- è¡Œæ•°: {stats['line_count']}
- æ®µè½æ•°: {stats['paragraph_count']}
- å¥å­æ•°: {stats['sentence_count']}

ğŸ“ è¯æ±‡ç»Ÿè®¡:
- ä¸­æ–‡å­—ç¬¦: {stats['chinese_char_count']} ä¸ª
- è‹±æ–‡å•è¯: {stats['english_word_count']} ä¸ª
- æ•°å­—: {stats['number_count']} ä¸ª
- æ€»è¯æ±‡å•å…ƒ: {total_words} ä¸ª
- å¹³å‡è¯æ±‡é•¿åº¦: {avg_word_length:.2f}

ğŸŒ è¯­è¨€æ„æˆ:
- ä¸­æ–‡æ¯”ä¾‹: {lang_ratios.get('chinese_ratio', 0):.1%}
- è‹±æ–‡æ¯”ä¾‹: {lang_ratios.get('english_ratio', 0):.1%}
- æ•°å­—æ¯”ä¾‹: {lang_ratios.get('number_ratio', 0):.1%}
- æ ‡ç‚¹æ¯”ä¾‹: {lang_ratios.get('punctuation_ratio', 0):.1%}"""

            # æ·»åŠ é«˜é¢‘è¯
            if word_freq:
                result += "\n\nğŸ”¥ é«˜é¢‘è¯æ±‡ (å‰5ä½):\n"
                for i, (word, count) in enumerate(word_freq, 1):
                    result += f"- {i}. '{word}': {count}æ¬¡\n"
            
            return result
            
        except Exception as e:
            return f"åˆ†æé”™è¯¯: {str(e)}"
    
    return Tool(
        name="text_analyzer",
        description="åˆ†ææ–‡æœ¬çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬ã€‚åŒ…æ‹¬å­—ç¬¦æ•°ã€è¯æ±‡æ•°ã€å¥å­æ•°ã€è¯­è¨€æ¯”ä¾‹ã€é«˜é¢‘è¯ç­‰ã€‚è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬å†…å®¹ã€‚",
        func=analyze_text,
        args_schema=None
    )


def create_text_sentiment_tool() -> Tool:
    """
    åˆ›å»ºæ–‡æœ¬æƒ…æ„Ÿåˆ†æå·¥å…·ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
    
    Returns:
        LangChain Toolå¯¹è±¡
    """
    def analyze_sentiment(text: str) -> str:
        """
        ç®€å•çš„æƒ…æ„Ÿåˆ†æ
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            
        Returns:
            æƒ…æ„Ÿåˆ†æç»“æœ
        """
        if not text or not text.strip():
            return "é”™è¯¯ï¼šè¯·æä¾›æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹"
        
        # ç®€å•çš„æƒ…æ„Ÿè¯å…¸ï¼ˆå®é™…ä½¿ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´ä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æåº“ï¼‰
        positive_words = [
            'å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢', 'çˆ±', 'ç¾', 'èµ', 'å®Œç¾', 'æ»¡æ„', 'å¼€å¿ƒ',
            'good', 'great', 'excellent', 'love', 'like', 'amazing', 'perfect', 'happy'
        ]
        
        negative_words = [
            'å', 'å·®', 'ç³Ÿç³•', 'è®¨åŒ', 'æ¨', 'éš¾çœ‹', 'å¤±æœ›', 'æ„¤æ€’', 'æ‚²ä¼¤', 'çƒ¦æ¼',
            'bad', 'terrible', 'awful', 'hate', 'ugly', 'disappointed', 'angry', 'sad'
        ]
        
        text_lower = text.lower()
        
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        # ç®€å•çš„æƒ…æ„Ÿåˆ¤æ–­
        if positive_count > negative_count:
            sentiment = "ç§¯æ ğŸ˜Š"
            confidence = positive_count / (positive_count + negative_count + 1)
        elif negative_count > positive_count:
            sentiment = "æ¶ˆæ ğŸ˜”"
            confidence = negative_count / (positive_count + negative_count + 1)
        else:
            sentiment = "ä¸­æ€§ ğŸ˜"
            confidence = 0.5
        
        return f"""ğŸ­ æƒ…æ„Ÿåˆ†æç»“æœ:

ğŸ“Š ç»Ÿè®¡:
- ç§¯æè¯æ±‡: {positive_count} ä¸ª
- æ¶ˆæè¯æ±‡: {negative_count} ä¸ª

ğŸ’­ æ•´ä½“æƒ…æ„Ÿ: {sentiment}
ğŸ“ˆ ç½®ä¿¡åº¦: {confidence:.1%}

æ³¨ï¼šè¿™æ˜¯åŸºäºç®€å•è¯å…¸çš„æƒ…æ„Ÿåˆ†æï¼Œç»“æœä»…ä¾›å‚è€ƒã€‚"""
    
    return Tool(
        name="sentiment_analyzer",
        description="åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼Œæ”¯æŒä¸­è‹±æ–‡ã€‚è¿”å›ç§¯æã€æ¶ˆææˆ–ä¸­æ€§çš„æƒ…æ„Ÿåˆ¤æ–­åŠç½®ä¿¡åº¦ã€‚",
        func=analyze_sentiment,
        args_schema=None
    )
